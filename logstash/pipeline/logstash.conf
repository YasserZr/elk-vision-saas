# Logstash pipeline configuration for processing log files

input {
  # TCP input for real-time log streaming
  tcp {
    port => 5000
    codec => json_lines
    tags => ["tcp"]
  }

  # UDP input for high-throughput scenarios
  udp {
    port => 5000
    codec => json_lines
    tags => ["udp"]
  }

  # Beats input for Filebeat/Metricbeat
  beats {
    port => 5044
    tags => ["beats"]
  }

  # HTTP input for REST API log submission
  http {
    port => 8080
    codec => json
    tags => ["http"]
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Extract timestamp from parsed JSON
  if [parsed][timestamp] {
    date {
      match => ["[parsed][timestamp]", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "yyyy-MM-dd HH:mm:ss"]
      target => "@timestamp"
      remove_field => ["[parsed][timestamp]"]
    }
  }

  # Handle direct @timestamp field (from backend parsers)
  if [@timestamp] and [@timestamp] !~ /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/ {
    date {
      match => ["@timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "yyyy-MM-dd HH:mm:ss"]
      target => "@timestamp"
    }
  }

  # Normalize log level
  if [parsed][level] {
    mutate {
      lowercase => ["[parsed][level]"]
    }
  } else if [level] {
    mutate {
      lowercase => ["level"]
    }
  }

  # Add GeoIP information if IP address is present
  if [parsed][ip] {
    geoip {
      source => "[parsed][ip]"
      target => "geoip"
    }
  }

  # Extract user agent information
  if [parsed][user_agent] {
    useragent {
      source => "[parsed][user_agent]"
      target => "user_agent"
    }
  }

  # Add default values
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "logs"
    }
  }

  # Categorize by log level
  if [parsed][level] in ["error", "critical", "fatal"] {
    mutate {
      add_tag => ["high_priority"]
    }
  }

  # Remove unnecessary fields including conflicting timestamp field
  mutate {
    remove_field => ["message", "timestamp", "_@timestamp"]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    user => "${ELASTICSEARCH_USERNAME:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    
    # Index template
    template_name => "logs"
    template_overwrite => true
  }

  # Output to stdout for debugging (disable in production)
  # stdout {
  #   codec => rubydebug
  # }
}
