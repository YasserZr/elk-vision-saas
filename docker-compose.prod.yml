version: '3.9'

# Production Docker Compose Configuration
# ========================================
# Optimized for production deployment with:
# - Resource limits
# - Health checks
# - Restart policies
# - Security hardening
# - Volume management

# Named volumes for persistent data
volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/elk-vision/postgres
  mongodb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/elk-vision/mongodb
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/elk-vision/redis
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/elk-vision/elasticsearch
  logstash_data:
    driver: local
  kibana_data:
    driver: local
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/elk-vision/prometheus
  grafana_data:
    driver: local
  static_files:
    driver: local
  media_files:
    driver: local
  certbot_conf:
    driver: local
  certbot_www:
    driver: local
  nginx_cache:
    driver: local

# Named networks with custom subnets
networks:
  frontend_network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET_FRONTEND:-172.20.0.0/24}
  backend_network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET_BACKEND:-172.21.0.0/24}
  elk_network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET_ELK:-172.23.0.0/24}
  database_network:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET_DATABASE:-172.22.0.0/24}
  monitoring_network:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET_MONITORING:-172.24.0.0/24}

services:
  # ============================================
  # PostgreSQL Database
  # ============================================
  postgres:
    image: postgres:15-alpine
    container_name: elk_postgres
    restart: always
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
      - /var/run/postgresql
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=en_US.UTF-8"
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - database_network
    ports:
      - "127.0.0.1:5432:5432"
    deploy:
      resources:
        limits:
          cpus: '${BACKEND_CPU_LIMIT:-2}'
          memory: ${POSTGRES_MEM_LIMIT:-2048m}
        reservations:
          memory: 1024m
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
      start_period: ${HEALTH_CHECK_START_PERIOD:-40s}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # MongoDB
  # ============================================
  mongodb:
    image: mongo:7.0
    container_name: elk_mongodb
    restart: always
    security_opt:
      - no-new-privileges:true
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DB_NAME}
    volumes:
      - mongodb_data:/data/db
      - ./mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro
    networks:
      - database_network
    ports:
      - "127.0.0.1:27017:27017"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: ${MONGODB_MEM_LIMIT:-2048m}
        reservations:
          memory: 1024m
    command: ["mongod", "--auth", "--bind_ip_all"]
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Redis
  # ============================================
  redis:
    image: redis:7-alpine
    container_name: elk_redis
    restart: always
    security_opt:
      - no-new-privileges:true
    command: >
      redis-server
      --appendonly yes
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --tcp-backlog 511
      --timeout 300
      --tcp-keepalive 60
    volumes:
      - redis_data:/data
    networks:
      - backend_network
      - database_network
    ports:
      - "127.0.0.1:6379:6379"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: ${REDIS_MEM_LIMIT:-512m}
        reservations:
          memory: 256m
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Elasticsearch
  # ============================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: elk_elasticsearch
    restart: always
    security_opt:
      - no-new-privileges:true
    environment:
      - node.name=${ELASTICSEARCH_NODE_NAME:-node-1}
      - cluster.name=${ELASTICSEARCH_CLUSTER_NAME:-elk-cluster}
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=${ES_JAVA_OPTS:--Xms2g -Xmx2g}"
      - xpack.security.enabled=${ELASTIC_SECURITY_ENABLED:-true}
      - xpack.security.enrollment.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - xpack.monitoring.collection.enabled=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - elk_network
      - backend_network
    ports:
      - "127.0.0.1:9200:9200"
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: ${ELASTICSEARCH_MEM_LIMIT:-4096m}
        reservations:
          memory: 2048m
    healthcheck:
      test: curl -s http://localhost:9200 >/dev/null || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Logstash
  # ============================================
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.1
    container_name: elk_logstash
    restart: always
    security_opt:
      - no-new-privileges:true
    environment:
      - "LS_JAVA_OPTS=${LS_JAVA_OPTS:--Xms512m -Xmx512m}"
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - LOGSTASH_MONITORING_ENABLED=true
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - logstash_data:/usr/share/logstash/data
    networks:
      - elk_network
      - backend_network
    ports:
      - "127.0.0.1:5000:5000"
      - "127.0.0.1:5044:5044"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1024m
        reservations:
          memory: 512m
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: curl -s http://localhost:9600 >/dev/null || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Kibana
  # ============================================
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    container_name: elk_kibana
    restart: always
    security_opt:
      - no-new-privileges:true
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - XPACK_SECURITY_ENABLED=true
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_SECURITY_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
    volumes:
      - kibana_data:/usr/share/kibana/data
      - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    networks:
      - elk_network
    ports:
      - "127.0.0.1:5601:5601"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1024m
        reservations:
          memory: 512m
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: curl -s http://localhost:5601/api/status >/dev/null || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Django Backend
  # ============================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
      args:
        - PYTHON_VERSION=3.11
    image: elk-vision/backend:${APP_VERSION:-latest}
    container_name: elk_backend
    restart: always
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    env_file:
      - .env.production
    environment:
      - ENVIRONMENT=production
      - DEBUG=False
    volumes:
      - static_files:/app/staticfiles:rw
      - media_files:/app/media:rw
      - /var/log/elk-vision:/var/log/elk-vision:rw
    networks:
      - backend_network
      - database_network
      - elk_network
    ports:
      - "127.0.0.1:8000:8000"
    deploy:
      resources:
        limits:
          cpus: '${BACKEND_CPU_LIMIT:-2}'
          memory: ${BACKEND_MEM_LIMIT:-2048m}
        reservations:
          memory: ${BACKEND_MEM_RESERVATION:-1024m}
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    depends_on:
      postgres:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    command: >
      sh -c "
        python manage.py migrate --noinput &&
        python manage.py collectstatic --noinput &&
        daphne -b 0.0.0.0 -p 8000 config.asgi:application
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Celery Worker
  # ============================================
  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    image: elk-vision/backend:${APP_VERSION:-latest}
    container_name: elk_celery_worker
    restart: always
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    env_file:
      - .env.production
    volumes:
      - /var/log/elk-vision:/var/log/elk-vision:rw
    networks:
      - backend_network
      - database_network
      - elk_network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2048m
        reservations:
          memory: 512m
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    depends_on:
      - redis
      - mongodb
      - elasticsearch
      - backend
    command: >
      celery -A config worker
      --loglevel=INFO
      --concurrency=${CELERY_WORKER_CONCURRENCY:-4}
      --max-tasks-per-child=${CELERY_WORKER_MAX_TASKS_PER_CHILD:-1000}
      --time-limit=${CELERY_TASK_TIME_LIMIT:-300}
      --soft-time-limit=${CELERY_TASK_SOFT_TIME_LIMIT:-240}
    healthcheck:
      test: celery -A config inspect ping
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Celery Beat
  # ============================================
  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    image: elk-vision/backend:${APP_VERSION:-latest}
    container_name: elk_celery_beat
    restart: always
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    env_file:
      - .env.production
    networks:
      - backend_network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512m
        reservations:
          memory: 256m
    depends_on:
      - redis
      - celery_worker
    command: celery -A config beat --loglevel=INFO --scheduler django_celery_beat.schedulers:DatabaseScheduler
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Next.js Frontend
  # ============================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL}
        - NEXT_PUBLIC_WS_URL=${NEXT_PUBLIC_WS_URL}
    image: elk-vision/frontend:${APP_VERSION:-latest}
    container_name: elk_frontend
    restart: always
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
      - /app/.next/cache
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL}
      - NEXT_PUBLIC_WS_URL=${NEXT_PUBLIC_WS_URL}
    networks:
      - frontend_network
    ports:
      - "127.0.0.1:3000:3000"
    deploy:
      resources:
        limits:
          cpus: '${FRONTEND_CPU_LIMIT:-1}'
          memory: ${FRONTEND_MEM_LIMIT:-1024m}
        reservations:
          memory: ${FRONTEND_MEM_RESERVATION:-512m}
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Nginx Reverse Proxy
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: elk_nginx
    restart: always
    security_opt:
      - no-new-privileges:true
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - static_files:/var/www/static:ro
      - media_files:/var/www/media:ro
      - certbot_conf:/etc/letsencrypt:ro
      - certbot_www:/var/www/certbot:ro
      - nginx_cache:/var/cache/nginx:rw
    networks:
      - frontend_network
      - backend_network
    ports:
      - "80:80"
      - "443:443"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512m
        reservations:
          memory: 256m
    depends_on:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Certbot for SSL Certificates
  # ============================================
  certbot:
    image: certbot/certbot:latest
    container_name: elk_certbot
    restart: "no"
    volumes:
      - certbot_conf:/etc/letsencrypt
      - certbot_www:/var/www/certbot
    networks:
      - frontend_network
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # ============================================
  # Flower - Celery Monitoring
  # ============================================
  flower:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    image: elk-vision/backend:${APP_VERSION:-latest}
    container_name: elk_flower
    restart: always
    security_opt:
      - no-new-privileges:true
    env_file:
      - .env.production
    networks:
      - backend_network
      - monitoring_network
    ports:
      - "127.0.0.1:5555:5555"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512m
    depends_on:
      - redis
      - celery_worker
    command: celery -A config flower --port=5555 --basic_auth=admin:${GRAFANA_ADMIN_PASSWORD}
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:5555/"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
